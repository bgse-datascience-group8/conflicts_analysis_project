---
title: "Synthesis pt1"
author: "Aimee Barciauskas"
date: "November 28, 2015"
output: pdf_document
---

After creating the table city_day_event_counts_plus (see database.md)

Data sub-selection by numbers:

* Created the table `city_day_event_counts_plus` (see database.md) which is table of number of conflict events per day per city in the U.S. between April 2013 and November 6, 2015.
* Started with 2,018 cities having conflicts since 2013, with a total of 1,507,041 events.
* Removed cities in Alaska and Hawaii
* Sub-select to 100 cities with the most data:
    * Summed the number of days per city represented, took the top 100
    * 82% of conflicts are represented by these top 100 cities
    * See `top_cities.sql`

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
library(RMySQL)
library(reshape)
library(ggplot2)
library(dplyr)
library(RColorBrewer)

images_dir <- '~/Box Sync/abarciausksas/myfiles/conflicts_analysis_project/conflict_analysis_app/public/images/'
rcolors <- brewer.pal(11, 'Spectral')
source('~/Box Sync/abarciausksas/myfiles/conflicts_analysis_project/scripts/synthesis/helpers.R')

con <- dbConnect(RMySQL::MySQL(), user="root", password="root", dbname = "gdelt")

res <- dbSendQuery(con, "select * from top_cities where sqldate > 20130701")
data <- dbFetch(res, n = -1)

# Standard data formatting
data$datef <- as.Date(as.character(data$sqldate), format = '%Y%m%d')
data$city <- paste0(data$name, ', ', data$admin1_code)
```

# Data standardization

Before standardization...

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
#install.packages('Rmisc')
library(Rmisc)

plot1 <- ggplot(data = data, aes(x = datef, y = num_conflicts, color = city)) + geom_line() +
  theme(
    legend.position = "none",
    panel.background = element_blank())

plot2 <- ggplot(data = data, aes(x = datef, y = log(num_conflicts), color = city)) + geom_line() +
  theme(
    legend.position = "none",
    panel.background = element_blank())

png(paste0(images_dir,'summary_num_conflicts_raw.png'), bg = "transparent", width=1200, height=1200, units='px')
multiplot(plot1, plot2, cols=1)
dev.off()
```

It's very noisy so we simplify the charts...

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
plot1 <- ggplot(data = data, aes(x = datef, y = num_conflicts, color = city)) + geom_smooth(method="loess", se=FALSE) + 
  scale_colour_manual(values=rep('#56B4E9',100)) +
  theme(legend.position = "none")

plot2 <- ggplot(data = data, aes(x = datef, y = log(num_conflicts), color = city)) + geom_smooth(method="loess", se=FALSE) + 
  scale_colour_manual(values=rep('#56B4E9',100)) +
  theme(legend.position = "none")

png(paste0(images_dir,'summary_num_conflicts_smooth.png'), bg = "transparent", width=1200, height=1200, units='px')
multiplot(plot1, plot2, cols=1)
dev.off()
```

[ADD ME]: Correlation between time and mean of data

Actually looks ok... may not need standardization

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
day_means <- tapply(data$num_conflicts, data$sqldate, FUN=mean)
day_means <- cbind(as.matrix(day_means), as.numeric(row.names(day_means)))
colnames(day_means) <- c('date_mean','sqldate')

day_sds <- tapply(data$num_conflicts, data$sqldate, FUN=sd)
day_sds <- cbind(as.matrix(day_sds), as.numeric(row.names(day_sds)))
colnames(day_sds) <- c('date_sd','sqldate')

data <- merge(data, day_means, by = 'sqldate', all.x = TRUE)
data <- merge(data, day_sds, by = 'sqldate', all.x = TRUE)

data$std_num_conflicts <- (data$num_conflicts - data$date_mean)/data$date_sd
```

After standardization...

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
plot <- ggplot(data = data, aes(x = datef, y = std_num_conflicts, color = city)) + geom_smooth(method="loess", se=FALSE) + 
  scale_colour_manual(values=rep('#56B4E9',100)) +
  theme(legend.position = "none")

png(paste0(images_dir,'summary_std_num_conflicts.png'), bg = "transparent", width=1200, height=1200, units='px')
plot
dev.off()
```

But adding a log(num_conflicts) column given data looks mostly stationary.

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
data$log_num_conflicts <- log(data$num_conflicts)
```

## Weekly trends

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
library(dplyr)
#create a vector of weekdays
weekdays1 <- c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday')

data$wday <- factor((weekdays(data$datef) %in% weekdays1), 
         levels=c(FALSE, TRUE), labels=c('weekend', 'weekday'))

start_date <- as.Date('20150901', format = '%Y%m%d')
end_date <- as.Date('20151031', format = '%Y%m%d')

fall2015 <- subset(data, datef > start_date & datef < end_date)

fall2015.sums <- aggregate(fall2015$num_conflicts, by=list(date=fall2015$datef,wday=fall2015$wday), FUN=sum)

plot <- ggplot(data = fall2015.sums, aes(y = x, x = date, fill=wday)) + geom_bar(stat='identity') + scale_fill_manual(values=c("#E69F00", "#56B4E9")) + xlab("Date") + ylab("Number of Conflicts") + ggtitle("Number of Conflicts by Day (Fall 2015)")

png(paste0(images_dir,'summary_weekly_trends.png'), bg = "transparent", width=1200, height=1200, units='px')
plot
dev.off()
```

# Auto-correlation

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
library(reshape2)
date_by_city_matrix <- data[,c('log_num_conflicts','datef','city')]

# fun aggregate needed as some cells are not represented -> 0
events_by_date_city <- dcast(data = date_by_city_matrix, formula = datef ~ city, value.var = 'log_num_conflicts', fun.aggregate = sum)

epsilon.t <- events_by_date_city
rownames(epsilon.t) <- epsilon.t$datef
epsilon.t <- epsilon.t[,setdiff(colnames(epsilon.t), 'datef')]
total_days <- length(unique(data$datef))
# Create epsilon.t.master
# For each feature id in unique feature ids, build it as above,
# cbind them all into a master matrix
# regress city day 0 on all columns except those that match '.day0'
ndays = 14
cities = colnames(epsilon.t)
epsilon.t.master = matrix(nrow = total_days-ndays+1)

for (idx in 1:length(cities)) {
  city <- cities[idx]
  epsilon.t.city <- embed(c(rep(NA, ndays-1),epsilon.t[,city]), ndays)
  colnames <- as.character(sapply(paste0(city, '.day'), paste0, 0:(ndays-1)))
  colnames(epsilon.t.city) <- colnames # Not that it matters
  epsilon.t.master <- cbind(epsilon.t.master, epsilon.t.city[ndays:nrow(epsilon.t.city),])
}

epsilon.t.master <- epsilon.t.master[,2:ncol(epsilon.t.master)]
```

## Example: Auto-correlation for a single city: Boston

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
boston <- epsilon.t.master[,c('Boston, MA.day0','Boston, MA.day1')]
y <- boston[,1]
X <- cbind(rep(1, nrow(boston)), boston[,2])
least_sqrs_beta <- my.lm(y,X)
# Compare with: model <- lm(y ~ 0+X)
least_sqrs_beta.fitted <- X %*% least_sqrs_beta
# Compare least_sqrs_beta.fitted with fitted.values(model)

least_sqrs_beta.residuals <- y - least_sqrs_beta.fitted

boston.df <- data.frame(boston[,1], least_sqrs_beta.fitted, least_sqrs_beta.residuals)
colnames(boston.df) <- c('actual','fitted','residuals')

plot <- ggplot(data=as.data.frame(boston), aes(x=`Boston, MA.day1`, y=`Boston, MA.day0`)) + geom_point(colour=rcolors[sample(1:11, 1)]) + geom_abline(aes(intercept = 0, slope = least_sqrs_beta), colour=rcolors[sample(1:11, 1)])

png(paste0(images_dir,'analysis_boston_ex1.png'), bg = "transparent", width=1200, height=1200, units='px')
plot
dev.off()

p1 <- ggplot(boston.df, aes(x=actual, y=fitted)) + geom_point(colour=rcolors[sample(1:11, 1)])

p2 <- ggplot(boston.df, aes(x=actual, y=residuals)) + geom_point(colour=rcolors[sample(1:11, 1)])

png(paste0(images_dir,'analysis_boston_ex2.png'), bg = "transparent", width=1200, height=1200, units='px')
multiplot(p1, p2, cols=2)
dev.off()
```


```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
# Evaluate auto-correlation by lagging n-days (unused)
pvalues_matrix <- matrix(ncol = 2, nrow = ncol(epsilon.t.master)/ndays)
colnames(pvalues_matrix) <- c('model','pvalue')

coeffs <- list()
current_indx <- 1
model_indx <- 1

while (current_indx < ncol(epsilon.t.master)) {
  # SELECT COLUMNS FOR CITY
  #current_indx <- 1
  cols_for_regression <- epsilon.t.master[,current_indx:(current_indx+(ndays-1))]

  # get coefficients
  betas <- my.lm(cols_for_regression[,1], cols_for_regression[,2:ndays])
  # ADD PVALUE AND MODEL TO MATRIX
  pvalues_matrix[model_indx,'model'] <- paste(colnames(cols_for_regression)[1], '~', paste(colnames(cols_for_regression)[2:ndays], collapse = ' + '))
  pvalues_matrix[model_indx,'pvalue'] <- my.pvalue(cols_for_regression[,1], cols_for_regression[,2:ndays])
  
  coeffs[[colnames(cols_for_regression)[1]]] = betas
                          
  # INCREMENT INDEX
  current_indx <- current_indx + ndays
  model_indx <- model_indx + 1
}

```

Evaluate auto-correlation by the nth-day lagged.

12 days selected for best effect-dropoff.

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
ndays <- 12
# coeffs matrix is ndays x ncities so we can plot the average coeff for each nday regression
coeffs <- matrix(nrow = ndays+1, ncol =length(unique(data$city)))
rownames(coeffs) <- 0:ndays
colnames(coeffs) <- cities

pvalues_matrix <- matrix(nrow = ndays, ncol =length(unique(data$city)))

# Reconstruct epsilon.t.master for the specific lag
for (i in 2:ndays) {
  nday <- i
  epsilon.t.master = matrix(nrow = total_days-nday+1)
  
  for (idx in 1:length(cities)) {
    city <- cities[idx]
    epsilon.t.city <- embed(c(rep(NA, nday-1),epsilon.t[,city]), nday)
    colnames <- as.character(sapply(paste0(city, '.day'), paste0, 0:(nday-1)))
    colnames(epsilon.t.city) <- colnames # Not that it matters
    epsilon.t.master <- cbind(epsilon.t.master, epsilon.t.city[nday:nrow(epsilon.t.city),])
  }
  
  epsilon.t.master <- epsilon.t.master[,2:ncol(epsilon.t.master)]
  
  current_indx <- 1
  model_indx <- 1
  
  while (current_indx < ncol(epsilon.t.master)) {
    # SELECT COLUMNS FOR CITY
    cols_for_regression <- epsilon.t.master[,c(current_indx, (current_indx+nday-1))]
  
    # RUN MODEL
    beta <- my.lm(cols_for_regression[,1], cols_for_regression[,2])
    # GET PVALUE
  
    # ADD PVALUE AND COEFFS TO MATRIX
    pvalues_matrix[nday,model_indx] <- my.pvalue(cols_for_regression[,1], as.matrix(cols_for_regression[,2]))
    coeffs[nday,model_indx] = beta
                            
    # INCREMENT INDEX
    current_indx <- current_indx + nday
    model_indx <- model_indx + 1
  }
}

coeffs <- coeffs[2:(nrow(coeffs)-1),]
pvalues <- pvalues_matrix[2:nrow(coeffs),]
lag_mean_eff <- rowMeans(coeffs)
pvalue_mean_val <- rowMeans(pvalues)

dat.m <- melt(coeffs, id.vars = "value")
plot <- ggplot(dat.m, aes(X1, value, color=factor(X2))) + geom_line(aes(group=X2)) + theme(legend.position='none')

png(paste0(images_dir,'analysis_lag_coeffs.png'), bg = "transparent", width=1200, height=1200, units='px')
plot
dev.off()

dat.m <- melt(coeffs, id.vars = "group")
plot <- ggplot(dat.m, aes(factor(X1), value)) + geom_boxplot()

png(paste0(images_dir,'analysis_lag_boxplot.png'), bg = "transparent", width=1200, height=1200, units='px')
plot
dev.off()
# FIXME: p-values are broken
#matplot(pvalue_mean_val, type = 'l', main = 'Mean p-values', xlab = 'nth-Day Lag', ylab = 'Average p-value')
```

## Neighbors correlation

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
library(lattice)
library(reshape2)
date_by_city_matrix <- data[,c('log_num_conflicts','datef','city')]

# fun aggregate needed as some cells are not represented -> 0
events_by_date_city <- dcast(data = date_by_city_matrix, formula = datef ~ city, value.var = 'log_num_conflicts', fun.aggregate = sum)

# Sanity check
# head(events_by_date_city[,'Tulsa, OK'], n = 20)
# head(data[data$city == 'Tulsa, OK',][,c('std_num_conflicts','datef')], n = 10)

# Make date column rownames
rownames(events_by_date_city) <- events_by_date_city$datef
events_by_date_city <- events_by_date_city[,setdiff(colnames(events_by_date_city), 'datef')]

X <- events_by_date_city

### HELPER FUNCTIONS
my.cov <- function(X) {
  n <- nrow(X)
  d <- data.frame(mean = colMeans(X))
  means <- t(do.call("cbind", replicate(n, d, simplify = FALSE)))
  diff_matrix <- as.matrix(X - means)
  1/(n-1) * t(diff_matrix) %*% diff_matrix
}
my.cor <- function(X) {
  cov.matrix <- my.cov(X)
  # expect a square matrix
  cor.matrix <- matrix(0, nrow = nrow(cov.matrix), ncol = nrow(cov.matrix))
  for (i in 1:ncol(cov.matrix)) {
    for (j in 1:nrow(cov.matrix)) {
      cor.matrix[i,j] <- cov.matrix[i,j]/(sqrt(cov.matrix[i,i])*sqrt(cov.matrix[j,j]))
    }
  }
  cor.matrix
}

X.cor <- my.cor(X)
X.m <- melt(X.cor)


png(paste0(images_dir,'analysis_network_heatmap.png'), bg = "transparent", width=1200, height=1200, units='px')
heatmap(X.cor, Rowv=NA, Colv=NA, scale="column", labRow=colnames(X), labCol=colnames(X),col = rev(heat.colors(256)))
dev.off()
```

## Neighborhoods

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
source('~/Box Sync/abarciausksas/myfiles/conflicts_analysis_project/scripts/data_expoloration/lasso.R')
X <- events_by_date_city
# for every column of X, want to run lasso on all other columns
# update coeff matrix, should be pxp
lasso.coeffs <- matrix(0, ncol=ncol(X), nrow = ncol(X))
rownames(lasso.coeffs) <- colnames(X)
colnames(lasso.coeffs) <- colnames(X)
for (i in 1:ncol(X)) {
  city <- colnames(X)[i]
  y <- as.numeric(X[,city])
  Xmat <- as.matrix(X[,setdiff(1:ncol(X), i)])
  model <- lm(y ~ Xmat)
  # When using a z-score, this was up to 5000
  coeffs <- lasso.reg(y, Xmat, 100)
  lasso.coeffs[city,setdiff(1:ncol(lasso.coeffs),i)] <- as.numeric(coeffs)
}

lasso.coeffs.select <- lasso.coeffs
# Remove them if they don't like each other (e.g. negative or 0 correlation)
for (i in 1:ncol(lasso.coeffs)) {
  for (j in 1:ncol(lasso.coeffs)) {
    if ((lasso.coeffs[i,j] <= 0.02) || (lasso.coeffs[j,i] <= 0.02)) {
      lasso.coeffs.select[i,j] = 0
      lasso.coeffs.select[j,i] = 0
    }
  }
}
```

[ADD ME: plot convergence]

[FIXME: Graph is too close together]

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
#install.packages("igraph")

## Load package
library(igraph)
s <- 1
f <- 99
lasso.part <- lasso.coeffs.select[s:f,s:f]
g <- graph.adjacency(lasso.part, weighted=T, mode = 'undirected')
g <- simplify(g)
V(g)$label <- colnames(events_by_date_city.v2[s:f,s:f])
V(g)$degree <- degree(g)
V(g)$size <- (V(g)$degree)

# interactive plot
#tkplot(g)
# FIXME: Too clustered together
layout <- layout.fruchterman.reingold
autocurve.edges(g)

png(paste0(images_dir,'analysis_network_basic_lasso.png'), bg = "transparent", width=1200, height=1200, units='px')
plot(g, layout=layout,
     vertex.label.family = "Helvetica",
     vertex.label.cex = 1.25,
     vertex.color='darkslategray3')
dev.off()

png(paste0(images_dir,'analysis_network_basic_lasso_hist.png'), bg = "transparent", width=1200, height=1200, units='px')
hist(V(g)$degree, 30, main='Degree of vertices', xlab='Degree')
dev.off()
```

[ADD ME: Degree distribution]

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
library(ggmap)
library(grid)

# For every pair of non-zero coeffs
# add city1.lat, city1.long 
network <- data.frame(
  'city.1.name' = character(),
  'city.1.lat' = numeric(),
  'city.1.long' = numeric(),
  'city.2.name' = character(),
  'city.2.lat' = numeric(),
  'city.2.long' = numeric(),
  stringsAsFactors=FALSE)

cities <- data.frame(
  'city.name' = character(),
  'city.lat' = numeric(),
  'city.long' = numeric(),
  stringsAsFactors=FALSE)
cities_idx <- 1

for (i in 1:ncol(lasso.coeffs.select)) {
  city <- colnames(lasso.coeffs.select)[i]
  neighbors <- names(which(lasso.coeffs[,city] != 0, arr.ind = T))
  city1.data <- data[data$city==city,][1,]
  
  if (!(city %in% cities$city.name)) {
    cities[cities_idx,] <- c(city,
      as.numeric(city1.data$latitude),
      as.numeric(city1.data$longitude))
    cities_idx <- cities_idx + 1
  }
  
  for (idx in 1:length(neighbors)) {
    city2 <- neighbors[idx]
    city2.data <- data[data$city==city2,][1,]
    new_row <- c(
      city,
      as.numeric(city1.data$latitude),
      as.numeric(city1.data$longitude),
      city2,
      as.numeric(city2.data$latitude),
      as.numeric(city2.data$longitude))
    network[(i+idx),] <- new_row
    
    if (!(city2 %in% cities$city.name)) {
      cities[cities_idx,] <- c(city2,
        as.numeric(city2.data$latitude),
        as.numeric(city2.data$longitude))
      cities_idx <- cities_idx + 1
    }
  }
}

network <- network[2:nrow(network),]
nrow(network)

map <- NULL
mapUS <- borders("state")
map <- ggplot() + mapUS
map <- map + 
  geom_segment(aes(
    y = as.numeric(network$city.1.lat),
    x = as.numeric(network$city.1.long),
    yend = as.numeric(network$city.2.lat),
    xend = as.numeric(network$city.2.long)),
    colour = 'steelblue2') +
  geom_text(data = cities, 
             aes(x = as.numeric(city.long), y = as.numeric(city.lat), label = city.name), colour = 'royalblue4', size = 3, vjust = .5, hjust = .8)

png(paste0(images_dir,'analysis_network_basic_lasso_map.png'), bg = "transparent", width=1200, height=1200, units='px')
map
dev.off()
```

[ADD ME: Calculate statistical significance of lasso.coeffs.select]

## Compare with SPACE results

```{r, echo = FALSE, warnings = FALSE, messages = FALSE}
library(space)

results <- space.joint(as.matrix(events_by_date_city),lam1=200)
P.hat   <- results$ParCor
N <- ncol(X)
A.hat   <- 1*( P.hat != 0 ) - diag(rep(1,N))
dimnames(A.hat) <- list(colnames(X), colnames(X))
g <- graph.adjacency(A.hat, weighted=T, mode = 'undirected')
g <- simplify(g)
V(g)$label <- colnames(events_by_date_city[s:f,s:f])
V(g)$degree <- degree(g)
V(g)$size <- V(g)$degree

# interactive plot
#tkplot(g)
layout <- layout.fruchterman.reingold

png(paste0(images_dir,'analysis_network_space.png'), bg = "transparent", width=1200, height=1200, units='px')
plot(g, layout=layout.sphere,
     vertex.label.family = "Helvetica",
     vertex.label.cex = 1,
     vertex.color='darkslategray3')
dev.off()

png(paste0(images_dir,'analysis_network_space_hist.png'), bg = "transparent", width=1200, height=1200, units='px')
hist(V(g)$degree, 30, main='Degree of vertices', xlab='Degree')
dev.off()
```

### Evalute std_num_conflicts v. avg:same_region + avg:other_regions

[ADD ME: Some summary of results]
[FIX ME: Regress on each region, so each region's effect is differentiated]

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }

# get the regions using k-means
cities <- data[,c('city','latitude','longitude')]
cities <- unique(cities)

bad_cities <- c('Moscow, 48','Honolulu, HI','Anchorage, AK','Fairbanks, AK')
cities <- subset(cities, !(city %in% bad_cities))

X <- as.matrix(cities[,c('longitude','latitude')])

nclusters <- 5
# FIXME: K-means is not clustering as well horizontally
source('~/Box Sync/abarciausksas/myfiles/conflicts_analysis_project/scripts/synthesis/kmeansGdist.R')
res <- k.means(X, nclusters)
r.nks <- res$r.nks
colnames(r.nks) <- sapply('region.', paste0, 1:nclusters)
cities <- cbind(cities, r.nks)

regions <- factor(apply(cities, 1, function(x) which(x == 1)), 
                    labels = colnames(cities)[4:(nclusters+4-1)]) 
cities <- cbind(cities, regions)
regions.list <- sort(as.character(unique(regions)))

library(ggmap)

map <- NULL
mapUS <- borders("state")
map <- ggplot() + mapUS
map <- map + 
  geom_text(data = cities, 
             aes(x = longitude, y = latitude, label = city, colour = regions), size = 5, vjust = .5, hjust = .8)

png(paste0(images_dir,'analysis_kluster_map.png'), bg = "transparent", width=1200, height=800, units='px')
map
dev.off()

# Matrix of coefficients for region effects for each city
# ncities x nregions + 1 (same region)
region_coeffs <- matrix(nrow=nrow(cities), ncol=length(regions.list)+1)
dimnames(region_coeffs) <- list(unique(cities$city), c('same_region', regions.list))

# for every city, calculate avg same_region and avg_other_regions
# doing counts so using glm - poisson
for (i in 1:nrow(cities)) {
  city <- cities[i,]
  region <- city$regions
  same_region_cities <- setdiff(cities[cities$regions == region,]$city, city$city)
  day_avgs_same_region <- rowMeans(events_by_date_city[,same_region_cities])

  region_avgs <- list()
  # get averages for other regions
  for (ridx in 1:length(regions.list)) {
    c_region <- regions.list[ridx]
    if (!c_region == as.character(region)) {
      c_region_cities <- cities[cities$regions == c_region,]
      avg_c_region <- rowMeans(events_by_date_city[,c_region_cities$city])
      region_avgs[[c_region]] <- avg_c_region
    }
  }
  region_avgs[['same_region']] = day_avgs_same_region
  
  region_avgs_df <- as.matrix(as.data.frame(region_avgs))
  model <- lm(events_by_date_city[,city$city] ~ 0+region_avgs_df)
  coeffs <- coefficients(model)
  names(coeffs) <- lapply(names(coeffs), function(n) { gsub("region_avgs_df", "", n) })

  for (ridx in 1:length(regions.list)) {
    c_region <- regions.list[ridx]
    region_coeffs[city$city,c_region] = coeffs[c_region]
  }
  region_coeffs[city$city,'same_region'] = coeffs['same_region']
}

png(paste0(images_dir,'analysis_kluster_region_coeffs_boxplot.png'), bg = "transparent", width=1200, height=1200, units='px')
boxplot(region_coeffs, use.cols = TRUE, ylim=c(-0.5,1.5))
dev.off()
```