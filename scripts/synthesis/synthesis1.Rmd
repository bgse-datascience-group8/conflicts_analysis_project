---
title: "Synthesis pt1"
author: "Aimee Barciauskas"
date: "November 28, 2015"
output: pdf_document
---

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
library(RMySQL)
library(reshape)
library(ggplot2)
library(dplyr)

con <- dbConnect(RMySQL::MySQL(), user="root", password="root", dbname = "gdelt")

# MySQL [gdelt]> select sum(num_conflicts) from city_day_event_counts_plus;
# +--------------------+
# | sum(num_conflicts) |
# +--------------------+
# |            1510380 |
# +--------------------+
# 1 row in set (0.08 sec)
# 
# MySQL [gdelt]> select sum(num_conflicts) from top_cities;
# +--------------------+
# | sum(num_conflicts) |
# +--------------------+
# |            1235960 |
# +--------------------+
# 1 row in set (0.03 sec)
res <- dbSendQuery(con, "select * from top_cities where sqldate > 20130701")
data <- dbFetch(res, n = -1)

# Standard data formatting
data$datef <- as.Date(as.character(data$sqldate), format = '%Y%m%d')
data$city <- paste0(data$name, ', ', data$admin1_code)

## Removing Washington, D.C. (for now)
data <- subset(data, geonameid != '4140963')
```

# Data standardization

Before standardization...

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
#install.packages('Rmisc')
library(Rmisc)

plot1 <- ggplot(data = data, aes(x = datef, y = num_conflicts, color = city)) + geom_line() +
  theme(legend.position = "none")

plot2 <- ggplot(data = data, aes(x = datef, y = log(num_conflicts), color = city)) + geom_line() +
  theme(legend.position = "none")

multiplot(plot1, plot2, cols=1)
```

It's very noisy so we simplify the charts...

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
plot1 <- ggplot(data = data, aes(x = datef, y = num_conflicts, color = city)) + geom_smooth(method="loess", se=FALSE) + 
  scale_colour_manual(values=rep('#56B4E9',99)) +
  theme(legend.position = "none")

plot2 <- ggplot(data = data, aes(x = datef, y = log(num_conflicts), color = city)) + geom_smooth(method="loess", se=FALSE) + 
  scale_colour_manual(values=rep('#56B4E9',99)) +
  theme(legend.position = "none")

multiplot(plot1, plot2, cols=1)
```

[ADD ME]: Correlation between time and mean of data

Actually looks ok... may not need standardization

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
day_means <- tapply(data$num_conflicts, data$sqldate, FUN=mean)
day_means <- cbind(as.matrix(day_means), as.numeric(row.names(day_means)))
colnames(day_means) <- c('date_mean','sqldate')

day_sds <- tapply(data$num_conflicts, data$sqldate, FUN=sd)
day_sds <- cbind(as.matrix(day_sds), as.numeric(row.names(day_sds)))
colnames(day_sds) <- c('date_sd','sqldate')

data <- merge(data, day_means, by = 'sqldate', all.x = TRUE)
data <- merge(data, day_sds, by = 'sqldate', all.x = TRUE)

data$std_num_conflicts <- (data$num_conflicts - data$date_mean)/data$date_sd
```

After standardization...

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
plot <- ggplot(data = data, aes(x = datef, y = std_num_conflicts, color = city)) + geom_smooth(method="loess", se=FALSE) + 
  scale_colour_manual(values=rep('#56B4E9',99)) +
  theme(legend.position = "none")
plot
```

But adding a log(num_conflicts) column given data looks mostly stationary.

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
data$log_num_conflicts <- log(data$num_conflicts)
```

## Weekly trends

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
library(dplyr)
#create a vector of weekdays
weekdays1 <- c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday')

data$wday <- factor((weekdays(data$datef) %in% weekdays1), 
         levels=c(FALSE, TRUE), labels=c('weekend', 'weekday'))

start_date <- as.Date('20150901', format = '%Y%m%d')
end_date <- as.Date('20151031', format = '%Y%m%d')

fall2015 <- subset(data, datef > start_date & datef < end_date)

fall2015.sums <- aggregate(fall2015$num_conflicts, by=list(date=fall2015$datef,wday=fall2015$wday), FUN=sum)

ggplot(data = fall2015.sums, aes(y = x, x = date, fill=wday)) + geom_bar(stat='identity') + scale_fill_manual(values=c("#E69F00", "#56B4E9")) + xlab("Date") + ylab("Number of Conflicts") + ggtitle("Number of Conflicts by Day (Fall 2015)")
```

# Auto-correlation

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
library(reshape2)
date_by_city_matrix <- data[,c('log_num_conflicts','datef','city')]

# fun aggregate needed as some cells are not represented -> 0
events_by_date_city <- dcast(data = date_by_city_matrix, formula = datef ~ city, value.var = 'log_num_conflicts', fun.aggregate = sum)

epsilon.t <- events_by_date_city
rownames(epsilon.t) <- epsilon.t$datef
epsilon.t <- epsilon.t[,setdiff(colnames(epsilon.t), 'datef')]
total_days <- length(unique(data$datef))
# Create epsilon.t.master
# For each feature id in unique feature ids, build it as above,
# cbind them all into a master matrix
# regress city day 0 on all columns except those that match '.day0'
ndays = 14
cities = colnames(epsilon.t)
epsilon.t.master = matrix(nrow = total_days-ndays+1)

for (idx in 1:length(cities)) {
  city <- cities[idx]
  epsilon.t.city <- embed(c(rep(NA, ndays-1),epsilon.t[,city]), ndays)
  colnames <- as.character(sapply(paste0(city, '.day'), paste0, 0:(ndays-1)))
  colnames(epsilon.t.city) <- colnames # Not that it matters
  epsilon.t.master <- cbind(epsilon.t.master, epsilon.t.city[ndays:nrow(epsilon.t.city),])
}

epsilon.t.master <- epsilon.t.master[,2:ncol(epsilon.t.master)]
```

## Example: Auto-correlation for a single city: Boston

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
boston <- epsilon.t.master[,c('Boston, MA.day0','Boston, MA.day1')]
least_sqrs_beta <- solve(t(boston[,2])%*%boston[,2]) %*% t(boston[,2]) %*% boston[,1]
# Compare with: model <- lm(boston[,1] ~ 0 + boston[,2])
least_sqrs_beta.fitted <- boston[,2] %*% least_sqrs_beta
# Compare least_sqrs_beta.fitted with fitted.values(model)

least_sqrs_beta.residuals <- boston[,1] - least_sqrs_beta.fitted
least_sqrs_beta.var <- (t(least_sqrs_beta.residuals)%*%least_sqrs_beta.residuals)/(nrow(boston)-1)
least_sqrs_beta.se <- sqrt(least_sqrs_beta.var * solve(t(boston[,2]) %*% boston[,2]))

# Wald
wald.stat <- (least_sqrs_beta - 0)/(least_sqrs_beta.se)
wald.pvalue <- function(wald.stat, df) {
  1 - pt(abs(wald.stat), df = df)
}
wald.pvalue(wald.stat, nrow(boston)-1)

plot(least_sqrs_beta.fitted, boston[,1], pch = 19, main = 'Fitted vs. Actual Values for 1-day lag in Boston', xlab = 'Fitted Value', ylab = 'Actual Value')
plot(least_sqrs_beta.residuals, pch = 19, main = 'Boston: 1-day Lag Model Residuals', ylab='Residuals', xlab='', xaxt='n')
```



```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
# Helper functions

my.lm <- function(y, X) solve(t(X)%*%X) %*% t(X) %*% y

my.rss <- function(y, X) {
  beta.hat <- my.lm(y, X)
  fitted.values <- X%*%beta.hat
  sum((y - fitted.values)**2)
}

my.tss <- function(y,X) {
  m <- mean(y)
  sum((y - m)**2)
}

my.ssm <- function(y,X) {
  beta.hat <- my.lm(y, X)
  fitted.values <- X%*%beta.hat
  sum((fitted.values - mean(y))**2)
}

my.pvalue <- function(y, X) {
  p <- ncol(X)
  n <- length(y)
  rss <- my.rss(y,X)
  # compare with: 
  # model<-lm(y ~ 0 + X)
  # sum(residuals(model)**2)
  ssm <- my.ssm(y,X)
  
  f.stat <- ((ssm-rss)/p)/(rss/(n - p))
  pvalue <- 1-pf(f.stat, df1=p, df2=n-p)
  
  return(pvalue)
}
```


```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
# Evaluate auto-correlation by lagging n-days (unused)
pvalues_matrix <- matrix(ncol = 2, nrow = ncol(epsilon.t.master)/ndays)
colnames(pvalues_matrix) <- c('model','pvalue')

coeffs <- list()
current_indx <- 1
model_indx <- 1

while (current_indx < ncol(epsilon.t.master)) {
  # SELECT COLUMNS FOR CITY
  #current_indx <- 1
  cols_for_regression <- epsilon.t.master[,current_indx:(current_indx+(ndays-1))]

  # get coefficients
  betas <- my.lm(cols_for_regression[,1], cols_for_regression[,2:ndays])
  # ADD PVALUE AND MODEL TO MATRIX
  pvalues_matrix[model_indx,'model'] <- paste(colnames(cols_for_regression)[1], '~', paste(colnames(cols_for_regression)[2:ndays], collapse = ' + '))
  pvalues_matrix[model_indx,'pvalue'] <- my.pvalue(cols_for_regression[,1], cols_for_regression[,2:ndays])
  
  coeffs[[colnames(cols_for_regression)[1]]] = betas
                          
  # INCREMENT INDEX
  current_indx <- current_indx + ndays
  model_indx <- model_indx + 1
}

```

Evaluate auto-correlation by the nth-day lagged.

12 days selected for best effect-dropoff.

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
ndays <- 12
# coeffs matrix is ndays x ncities so we can plot the average coeff for each nday regression
coeffs <- matrix(nrow = ndays, ncol =length(unique(data$city)))
pvalues_matrix <- matrix(nrow = ndays, ncol =length(unique(data$city)))

# Reconstruct epsilon.t.master for the specific lag
for (i in 2:ndays) {
  nday <- i
  epsilon.t.master = matrix(nrow = total_days-nday+1)
  
  for (idx in 1:length(cities)) {
    city <- cities[idx]
    epsilon.t.city <- embed(c(rep(NA, nday-1),epsilon.t[,city]), nday)
    colnames <- as.character(sapply(paste0(city, '.day'), paste0, 0:(nday-1)))
    colnames(epsilon.t.city) <- colnames # Not that it matters
    epsilon.t.master <- cbind(epsilon.t.master, epsilon.t.city[nday:nrow(epsilon.t.city),])
  }
  
  epsilon.t.master <- epsilon.t.master[,2:ncol(epsilon.t.master)]
  
  current_indx <- 1
  model_indx <- 1
  
  while (current_indx < ncol(epsilon.t.master)) {
    # SELECT COLUMNS FOR CITY
    cols_for_regression <- epsilon.t.master[,c(current_indx, (current_indx+nday-1))]
  
    # RUN MODEL
    beta <- my.lm(cols_for_regression[,1], cols_for_regression[,2])
    # GET PVALUE
  
    # ADD PVALUE AND COEFFS TO MATRIX
    pvalues_matrix[nday,model_indx] <- my.pvalue(cols_for_regression[,1], as.matrix(cols_for_regression[,2]))
    coeffs[nday,model_indx] = beta
                            
    # INCREMENT INDEX
    current_indx <- current_indx + nday
    model_indx <- model_indx + 1
  }
}
coeffs <- coeffs[2:nrow(coeffs),]
pvalues <- pvalues_matrix[2:nrow(coeffs),]
lag_mean_eff <- rowMeans(coeffs)
pvalue_mean_val <- rowMeans(pvalues)

library(scales)

matplot(coeffs, type='l', xlab = "nth-Day Lag", ylab = 'Coefficient', main = 'Auto correlation: Effect by day and city', ylim = c(0,1.2))

matplot(lag_mean_eff, type='l', xlab = "nth-Day Lag", ylab = 'Average Coefficient', main = 'Auto correlation: Average effect by day', ylim = c(0.5,0.75))


# FIXME: p-values are broken
#matplot(pvalue_mean_val, type = 'l', main = 'Mean p-values', xlab = 'nth-Day Lag', ylab = 'Average p-value')
```

## Neighbors correlation

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
library(lattice)
library(reshape2)
date_by_city_matrix <- data[,c('log_num_conflicts','datef','city')]

# fun aggregate needed as some cells are not represented -> 0
events_by_date_city <- dcast(data = date_by_city_matrix, formula = datef ~ city, value.var = 'log_num_conflicts', fun.aggregate = sum)

# Sanity check
# head(events_by_date_city[,'Tulsa, OK'], n = 20)
# head(data[data$city == 'Tulsa, OK',][,c('std_num_conflicts','datef')], n = 10)

# Make date column rownames
rownames(events_by_date_city) <- events_by_date_city$datef
events_by_date_city <- events_by_date_city[,setdiff(colnames(events_by_date_city), 'datef')]

X <- events_by_date_city

### HELPER FUNCTIONS
my.cov <- function(X) {
  n <- nrow(X)
  d <- data.frame(mean = colMeans(X))
  means <- t(do.call("cbind", replicate(n, d, simplify = FALSE)))
  diff_matrix <- as.matrix(X - means)
  1/(n-1) * t(diff_matrix) %*% diff_matrix
}
my.cor <- function(X) {
  cov.matrix <- my.cov(X)
  # expect a square matrix
  cor.matrix <- matrix(0, nrow = nrow(cov.matrix), ncol = nrow(cov.matrix))
  for (i in 1:ncol(cov.matrix)) {
    for (j in 1:nrow(cov.matrix)) {
      cor.matrix[i,j] <- cov.matrix[i,j]/(sqrt(cov.matrix[i,i])*sqrt(cov.matrix[j,j]))
    }
  }
  cor.matrix
}

X.cor <- my.cor(X)
X.m <- melt(X.cor)
heatmap(X.cor, scale="column", labRow=colnames(X), labCol=colnames(X),col = rev(heat.colors(256)))
```

## Neighborhoods

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
source('~/Box Sync/abarciausksas/myfiles/conflicts_analysis_project/scripts/data_expoloration/lasso.R')
X <- events_by_date_city
# for every column of X, want to run lasso on all other columns
# update coeff matrix, should be pxp
lasso.coeffs <- matrix(0, ncol=ncol(X), nrow = ncol(X))
rownames(lasso.coeffs) <- colnames(X)
colnames(lasso.coeffs) <- colnames(X)
for (i in 1:ncol(X)) {
  city <- colnames(X)[i]
  y <- as.numeric(X[,city])
  Xmat <- as.matrix(X[,setdiff(1:ncol(X), i)])
  model <- lm(y ~ Xmat)
  # When using a z-score, this was up to 5000
  coeffs <- lasso.reg(y, Xmat, 100)
  lasso.coeffs[city,setdiff(1:ncol(lasso.coeffs),i)] <- as.numeric(coeffs)
}

lasso.coeffs.select <- lasso.coeffs
# Remove them if they don't like each other (e.g. negative or 0 correlation)
for (i in 1:ncol(lasso.coeffs)) {
  for (j in 1:ncol(lasso.coeffs)) {
    if ((lasso.coeffs[i,j] <= 0.02) || (lasso.coeffs[j,i] <= 0.02)) {
      lasso.coeffs.select[i,j] = 0
      lasso.coeffs.select[j,i] = 0
    }
  }
}
```

[ADD ME: plot convergence]

[FIXME: Graph is too close together]

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }
#install.packages("igraph")

## Load package
library(igraph)
s <- 1
f <- 99
lasso.part <- lasso.coeffs.select[s:f,s:f]
g <- graph.adjacency(lasso.part, weighted=T, mode = 'undirected')
g <- simplify(g)
V(g)$label <- colnames(events_by_date_city.v2[s:f,s:f])
V(g)$degree <- degree(g)
V(g)$size <- (V(g)$degree)

# interactive plot
#tkplot(g)
# FIXME: Too clustered together
layout <- layout.fruchterman.reingold
autocurve.edges(g)
plot(g, layout=layout,
     vertex.label.family = "Helvetica",
     vertex.label.cex = 0.5,
     vertex.color='darkslategray3')

hist(V(g)$degree, 30, main='Degree of vertices', xlab='Degree')
```

[ADD ME: Degree distribution]

```{r, echo=FALSE, warnings=FALSE, messages=FALSE}
library(ggmap)
library(grid)

# For every pair of non-zero coeffs
# add city1.lat, city1.long 
network <- data.frame(
  'city.1.name' = character(),
  'city.1.lat' = numeric(),
  'city.1.long' = numeric(),
  'city.2.name' = character(),
  'city.2.lat' = numeric(),
  'city.2.long' = numeric(),
  stringsAsFactors=FALSE)

cities <- data.frame(
  'city.name' = character(),
  'city.lat' = numeric(),
  'city.long' = numeric(),
  stringsAsFactors=FALSE)
cities_idx <- 1

for (i in 1:ncol(lasso.coeffs.select)) {
  city <- colnames(lasso.coeffs.select)[i]
  neighbors <- names(which(lasso.coeffs[,city] != 0, arr.ind = T))
  city1.data <- data[data$city==city,][1,]
  
  if (!(city %in% cities$city.name)) {
    cities[cities_idx,] <- c(city,
      as.numeric(city1.data$latitude),
      as.numeric(city1.data$longitude))
    cities_idx <- cities_idx + 1
  }
  
  for (idx in 1:length(neighbors)) {
    city2 <- neighbors[idx]
    city2.data <- data[data$city==city2,][1,]
    new_row <- c(
      city,
      as.numeric(city1.data$latitude),
      as.numeric(city1.data$longitude),
      city2,
      as.numeric(city2.data$latitude),
      as.numeric(city2.data$longitude))
    network[(i+idx),] <- new_row
    
    if (!(city2 %in% cities$city.name)) {
      cities[cities_idx,] <- c(city2,
        as.numeric(city2.data$latitude),
        as.numeric(city2.data$longitude))
      cities_idx <- cities_idx + 1
    }
  }
}

network <- network[2:nrow(network),]
nrow(network)
bad_cities <- c('Moscow, 48','Honolulu, HI','Anchorage, AK','Fairbanks, AK')
network <- subset(network, !(city.1.name %in% bad_cities))
network <- subset(network, !(city.2.name %in% bad_cities))
cities <- subset(cities, !(city.name %in% bad_cities))

map <- NULL
mapUS <- borders("state")
map <- ggplot() + mapUS
map <- map + 
  geom_segment(aes(
    y = as.numeric(network$city.1.lat),
    x = as.numeric(network$city.1.long),
    yend = as.numeric(network$city.2.lat),
    xend = as.numeric(network$city.2.long)),
    colour = 'steelblue2') +
  geom_text(data = cities, 
             aes(x = as.numeric(city.long), y = as.numeric(city.lat), label = city.name), colour = 'royalblue4', size = 3, vjust = .5, hjust = .8)
map
```

[ADD ME: Calculate statistical significance of lasso.coeffs.select]

## Compare with SPACE results

```{r, echo = FALSE, warnings = FALSE, messages = FALSE}
library(space)

results <- space.joint(as.matrix(events_by_date_city),lam1=300)
P.hat   <- results$ParCor
N <- ncol(X)
A.hat   <- 1*( P.hat != 0 ) - diag(rep(1,N))
dimnames(A.hat) <- list(colnames(X), colnames(X))
g <- graph.adjacency(A.hat, weighted=T, mode = 'undirected')
g <- simplify(g)
V(g)$label <- colnames(events_by_date_city[s:f,s:f])
V(g)$degree <- degree(g)
V(g)$size <- (V(g)$degree)
hist(V(g)$degree, 30, main='Degree of vertices', xlab='Degree')

# interactive plot
#tkplot(g)
layout <- layout.fruchterman.reingold
autocurve.edges(g)
plot(g, layout=layout,
     vertex.label.family = "Helvetica",
     vertex.label.cex = 0.5,
     vertex.color='darkslategray3')
```

### Evalute std_num_conflicts v. avg:same_region + avg:other_regions

[ADD ME: Some summary of results]
[FIX ME: Regress on each region, so each region's effect is differentiated]

```{r, echo=FALSE, warnings=FALSE, messages=FALSE }

# get the regions using k-means
cities <- data[,c('city','latitude','longitude')]
cities <- unique(cities)

bad_cities <- c('Moscow, 48','Honolulu, HI','Anchorage, AK','Fairbanks, AK')
cities <- subset(cities, !(city %in% bad_cities))

X <- as.matrix(cities[,c('latitude','longitude')])

nclusters <- 6

source('~/Box Sync/abarciausksas/myfiles/conflicts_analysis_project/scripts/synthesis/kmeans.R')
res <- k.means(X, nclusters)
r.nks <- res$r.nks
colnames(r.nks) <- sapply('region.', paste0, 1:nclusters)
cities <- cbind(cities, r.nks)

regions <- factor(apply(cities, 1, function(x) which(x == 1)), 
                    labels = colnames(cities)[4:9]) 
cities <- cbind(cities, regions)
regions.list <- sort(as.character(unique(regions)))

library(ggmap)

map <- NULL
mapUS <- borders("state")
map <- ggplot() + mapUS
map <- map + 
  geom_text(data = cities, 
             aes(x = longitude, y = latitude, label = city, colour = regions), size = 3, vjust = .5, hjust = .8)
map

# Matrix of coefficients for region effects for each city
# ncities x nregions + 1 (same region)
region_coeffs <- matrix(nrow=nrow(cities), ncol=length(regions.list)+1)
dimnames(region_coeffs) <- list(unique(cities$city), c('same_region', regions.list))

# for every city, calculate avg same_region and avg_other_regions
# doing counts so using glm - poisson
for (i in 1:nrow(cities)) {
  city <- cities[i,]
  region <- city$regions
  same_region_cities <- setdiff(cities[cities$regions == region,]$city, city$city)
  day_avgs_same_region <- rowMeans(events_by_date_city[,same_region_cities])

  region_avgs <- list()
  # get averages for other regions
  for (ridx in 1:length(regions.list)) {
    c_region <- regions.list[ridx]
    if (!c_region == as.character(region)) {
      c_region_cities <- cities[cities$regions == c_region,]
      avg_c_region <- rowMeans(events_by_date_city[,c_region_cities$city])
      region_avgs[[c_region]] <- avg_c_region
    }
  }
  region_avgs[['same_region']] = day_avgs_same_region
  
  region_avgs_df <- as.matrix(as.data.frame(region_avgs))
  model <- lm(events_by_date_city[,city$city] ~ 0+region_avgs_df)
  coeffs <- coefficients(model)
  names(coeffs) <- lapply(names(coeffs), function(n) { gsub("region_avgs_df", "", n) })

  for (ridx in 1:length(regions.list)) {
    c_region <- regions.list[ridx]
    region_coeffs[city$city,c_region] = coeffs[c_region]
  }
  region_coeffs[city$city,'same_region'] = coeffs['same_region']
}

colors = brewer.pal(length(regions.list), 'RdYlBu')
plot(region_coeffs[,'same_region'], col=colors[1], type='l')
for (ridx in 1:length(regions.list)) {
  lines(region_coeffs[,regions.list[ridx]], col=colors[ridx+1], type='l')
}

boxplot(region_coeffs, use.cols = TRUE)
```